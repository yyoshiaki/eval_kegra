{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base : train.py\n",
    "# data : cora\n",
    "\n",
    "元データはhttps://linqs.soe.ucsc.edu/dataにある\n",
    "- content\n",
    "- cites\n",
    "の2つのファイル\n",
    "\n",
    "1.cites\n",
    "<ID of cited paper> <ID of citing paper>\n",
    "引用されている論文のID、引用している論文のID\n",
    "5430のエッジ\n",
    "\n",
    "2.content\n",
    "<paper_id> <word_attributes>+ <class_label>\n",
    "2708x1435の行列\n",
    "論文ID+単語(1433)+class\n",
    "(例)31336, 0,0,1,…,0,Neural_Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import *\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "DATASET = 'cora'\n",
    "FILTER = 'localpool'  # 'chebyshev'\n",
    "MAX_DEGREE = 2  # maximum polynomial degree\n",
    "SYM_NORM = True  # symmetric (True) vs. left-only (False) normalization\n",
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X, A, y = load_data(dataset=DATASET)\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)\n",
    "\n",
    "# Normalize X\n",
    "X /= X.sum(1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mget_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.6/site-packages/kegra-0.0.1-py3.6.egg/kegra/utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 1433)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 2708)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(500, 1500)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ↑この辺の処理（split）がよくわからん"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local pooling filters...\n"
     ]
    }
   ],
   "source": [
    "if FILTER == 'localpool':\n",
    "    \"\"\" Local pooling filters (see 'renormalization trick' in Kipf & Welling, arXiv 2016) \"\"\"\n",
    "    print('Using local pooling filters...')\n",
    "    A_ = preprocess_adj(A, SYM_NORM)\n",
    "    support = 1\n",
    "    graph = [X, A_]\n",
    "    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]\n",
    "\n",
    "elif FILTER == 'chebyshev':\n",
    "    \"\"\" Chebyshev polynomial basis filters (Defferard et al., NIPS 2016)  \"\"\"\n",
    "    print('Using Chebyshev polynomial basis filters...')\n",
    "    L = normalized_laplacian(A, SYM_NORM)\n",
    "    L_scaled = rescale_laplacian(L)\n",
    "    T_k = chebyshev_polynomial(L_scaled, MAX_DEGREE)\n",
    "    support = MAX_DEGREE + 1\n",
    "    graph = [X]+T_k\n",
    "    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(support)]\n",
    "    \n",
    "\n",
    "else:\n",
    "    raise Exception('Invalid filter type.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_in = Input(shape=(X.shape[1],))\n",
    "\n",
    "# Define model architecture\n",
    "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 1433)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 1433)          0           input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_1 (InputLayer)             (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "graph_convolution_1 (GraphConvol (None, 16)            22944       dropout_1[0][0]                  \n",
      "                                                                   input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 16)            0           graph_convolution_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "graph_convolution_2 (GraphConvol (None, 7)             119         dropout_2[0][0]                  \n",
      "                                                                   input_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 23,063\n",
      "Trainable params: 23,063\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9343 train_acc= 0.3571 val_loss= 1.9363 val_acc= 0.3300 time= 1.4581\n",
      "Epoch: 0002 train_loss= 1.9241 train_acc= 0.2286 val_loss= 1.9287 val_acc= 0.1800 time= 0.0169\n",
      "Epoch: 0003 train_loss= 1.9132 train_acc= 0.2214 val_loss= 1.9203 val_acc= 0.1733 time= 0.0170\n",
      "Epoch: 0004 train_loss= 1.9013 train_acc= 0.2429 val_loss= 1.9112 val_acc= 0.1933 time= 0.0170\n",
      "Epoch: 0005 train_loss= 1.8885 train_acc= 0.2786 val_loss= 1.9015 val_acc= 0.2233 time= 0.0168\n",
      "Epoch: 0006 train_loss= 1.8756 train_acc= 0.2786 val_loss= 1.8919 val_acc= 0.2167 time= 0.0163\n",
      "Epoch: 0007 train_loss= 1.8624 train_acc= 0.2857 val_loss= 1.8821 val_acc= 0.2233 time= 0.0167\n",
      "Epoch: 0008 train_loss= 1.8488 train_acc= 0.2929 val_loss= 1.8718 val_acc= 0.2200 time= 0.0166\n",
      "Epoch: 0009 train_loss= 1.8352 train_acc= 0.3214 val_loss= 1.8616 val_acc= 0.2333 time= 0.0168\n",
      "Epoch: 0010 train_loss= 1.8217 train_acc= 0.3286 val_loss= 1.8514 val_acc= 0.2533 time= 0.0171\n",
      "Epoch: 0011 train_loss= 1.8081 train_acc= 0.3571 val_loss= 1.8411 val_acc= 0.2800 time= 0.0181\n",
      "Epoch: 0012 train_loss= 1.7947 train_acc= 0.3714 val_loss= 1.8311 val_acc= 0.3067 time= 0.0187\n",
      "Epoch: 0013 train_loss= 1.7814 train_acc= 0.3714 val_loss= 1.8214 val_acc= 0.3200 time= 0.0174\n",
      "Epoch: 0014 train_loss= 1.7686 train_acc= 0.4000 val_loss= 1.8118 val_acc= 0.3500 time= 0.0215\n",
      "Epoch: 0015 train_loss= 1.7562 train_acc= 0.4214 val_loss= 1.8027 val_acc= 0.3733 time= 0.0178\n",
      "Epoch: 0016 train_loss= 1.7441 train_acc= 0.4286 val_loss= 1.7941 val_acc= 0.3867 time= 0.0169\n",
      "Epoch: 0017 train_loss= 1.7323 train_acc= 0.4357 val_loss= 1.7854 val_acc= 0.4067 time= 0.0164\n",
      "Epoch: 0018 train_loss= 1.7209 train_acc= 0.4571 val_loss= 1.7772 val_acc= 0.4300 time= 0.0184\n",
      "Epoch: 0019 train_loss= 1.7098 train_acc= 0.4571 val_loss= 1.7690 val_acc= 0.4367 time= 0.0164\n",
      "Epoch: 0020 train_loss= 1.6988 train_acc= 0.4786 val_loss= 1.7606 val_acc= 0.4367 time= 0.0163\n",
      "Epoch: 0021 train_loss= 1.6880 train_acc= 0.4786 val_loss= 1.7524 val_acc= 0.4467 time= 0.0158\n",
      "Epoch: 0022 train_loss= 1.6771 train_acc= 0.4857 val_loss= 1.7442 val_acc= 0.4600 time= 0.0158\n",
      "Epoch: 0023 train_loss= 1.6664 train_acc= 0.4929 val_loss= 1.7359 val_acc= 0.4633 time= 0.0162\n",
      "Epoch: 0024 train_loss= 1.6557 train_acc= 0.4929 val_loss= 1.7276 val_acc= 0.4733 time= 0.0165\n",
      "Epoch: 0025 train_loss= 1.6450 train_acc= 0.4786 val_loss= 1.7194 val_acc= 0.4633 time= 0.0161\n",
      "Epoch: 0026 train_loss= 1.6343 train_acc= 0.4714 val_loss= 1.7114 val_acc= 0.4633 time= 0.0197\n",
      "Epoch: 0027 train_loss= 1.6235 train_acc= 0.4714 val_loss= 1.7034 val_acc= 0.4533 time= 0.0163\n",
      "Epoch: 0028 train_loss= 1.6126 train_acc= 0.4643 val_loss= 1.6956 val_acc= 0.4500 time= 0.0166\n",
      "Epoch: 0029 train_loss= 1.6018 train_acc= 0.4643 val_loss= 1.6879 val_acc= 0.4467 time= 0.0167\n",
      "Epoch: 0030 train_loss= 1.5908 train_acc= 0.4643 val_loss= 1.6801 val_acc= 0.4467 time= 0.0160\n",
      "Epoch: 0031 train_loss= 1.5797 train_acc= 0.4643 val_loss= 1.6723 val_acc= 0.4467 time= 0.0165\n",
      "Epoch: 0032 train_loss= 1.5684 train_acc= 0.4714 val_loss= 1.6646 val_acc= 0.4433 time= 0.0161\n",
      "Epoch: 0033 train_loss= 1.5571 train_acc= 0.4786 val_loss= 1.6570 val_acc= 0.4433 time= 0.0166\n",
      "Epoch: 0034 train_loss= 1.5457 train_acc= 0.4857 val_loss= 1.6496 val_acc= 0.4433 time= 0.0171\n",
      "Epoch: 0035 train_loss= 1.5344 train_acc= 0.4857 val_loss= 1.6421 val_acc= 0.4433 time= 0.0167\n",
      "Epoch: 0036 train_loss= 1.5229 train_acc= 0.4857 val_loss= 1.6348 val_acc= 0.4433 time= 0.0163\n",
      "Epoch: 0037 train_loss= 1.5113 train_acc= 0.4857 val_loss= 1.6275 val_acc= 0.4467 time= 0.0162\n",
      "Epoch: 0038 train_loss= 1.4996 train_acc= 0.4857 val_loss= 1.6202 val_acc= 0.4533 time= 0.0182\n",
      "Epoch: 0039 train_loss= 1.4877 train_acc= 0.5071 val_loss= 1.6128 val_acc= 0.4600 time= 0.0162\n",
      "Epoch: 0040 train_loss= 1.4758 train_acc= 0.5071 val_loss= 1.6051 val_acc= 0.4600 time= 0.0163\n",
      "Epoch: 0041 train_loss= 1.4639 train_acc= 0.5214 val_loss= 1.5973 val_acc= 0.4700 time= 0.0162\n",
      "Epoch: 0042 train_loss= 1.4519 train_acc= 0.5214 val_loss= 1.5891 val_acc= 0.4833 time= 0.0164\n",
      "Epoch: 0043 train_loss= 1.4399 train_acc= 0.5357 val_loss= 1.5805 val_acc= 0.4933 time= 0.0161\n",
      "Epoch: 0044 train_loss= 1.4279 train_acc= 0.5429 val_loss= 1.5719 val_acc= 0.4967 time= 0.0166\n",
      "Epoch: 0045 train_loss= 1.4159 train_acc= 0.5500 val_loss= 1.5632 val_acc= 0.5033 time= 0.0175\n",
      "Epoch: 0046 train_loss= 1.4040 train_acc= 0.5786 val_loss= 1.5546 val_acc= 0.5067 time= 0.0161\n",
      "Epoch: 0047 train_loss= 1.3921 train_acc= 0.6000 val_loss= 1.5461 val_acc= 0.5267 time= 0.0166\n",
      "Epoch: 0048 train_loss= 1.3803 train_acc= 0.6000 val_loss= 1.5378 val_acc= 0.5267 time= 0.0159\n",
      "Epoch: 0049 train_loss= 1.3686 train_acc= 0.6000 val_loss= 1.5292 val_acc= 0.5333 time= 0.0162\n",
      "Epoch: 0050 train_loss= 1.3571 train_acc= 0.6071 val_loss= 1.5204 val_acc= 0.5400 time= 0.0193\n",
      "Epoch: 0051 train_loss= 1.3457 train_acc= 0.6143 val_loss= 1.5120 val_acc= 0.5433 time= 0.0158\n",
      "Epoch: 0052 train_loss= 1.3343 train_acc= 0.6357 val_loss= 1.5038 val_acc= 0.5467 time= 0.0164\n",
      "Epoch: 0053 train_loss= 1.3230 train_acc= 0.6357 val_loss= 1.4955 val_acc= 0.5467 time= 0.0158\n",
      "Epoch: 0054 train_loss= 1.3118 train_acc= 0.6500 val_loss= 1.4873 val_acc= 0.5500 time= 0.0165\n",
      "Epoch: 0055 train_loss= 1.3006 train_acc= 0.6714 val_loss= 1.4793 val_acc= 0.5600 time= 0.0159\n",
      "Epoch: 0056 train_loss= 1.2895 train_acc= 0.6786 val_loss= 1.4715 val_acc= 0.5667 time= 0.0163\n",
      "Epoch: 0057 train_loss= 1.2783 train_acc= 0.7000 val_loss= 1.4638 val_acc= 0.5833 time= 0.0159\n",
      "Epoch: 0058 train_loss= 1.2674 train_acc= 0.7286 val_loss= 1.4562 val_acc= 0.5933 time= 0.0169\n",
      "Epoch: 0059 train_loss= 1.2567 train_acc= 0.7286 val_loss= 1.4486 val_acc= 0.6067 time= 0.0171\n",
      "Epoch: 0060 train_loss= 1.2459 train_acc= 0.7286 val_loss= 1.4408 val_acc= 0.6200 time= 0.0159\n",
      "Epoch: 0061 train_loss= 1.2351 train_acc= 0.7357 val_loss= 1.4330 val_acc= 0.6267 time= 0.0158\n",
      "Epoch: 0062 train_loss= 1.2243 train_acc= 0.7429 val_loss= 1.4248 val_acc= 0.6267 time= 0.0160\n",
      "Epoch: 0063 train_loss= 1.2137 train_acc= 0.7429 val_loss= 1.4166 val_acc= 0.6267 time= 0.0163\n",
      "Epoch: 0064 train_loss= 1.2035 train_acc= 0.7429 val_loss= 1.4084 val_acc= 0.6267 time= 0.0168\n",
      "Epoch: 0065 train_loss= 1.1935 train_acc= 0.7214 val_loss= 1.4000 val_acc= 0.6267 time= 0.0168\n",
      "Epoch: 0066 train_loss= 1.1836 train_acc= 0.7214 val_loss= 1.3919 val_acc= 0.6267 time= 0.0164\n",
      "Epoch: 0067 train_loss= 1.1736 train_acc= 0.7214 val_loss= 1.3841 val_acc= 0.6300 time= 0.0160\n",
      "Epoch: 0068 train_loss= 1.1636 train_acc= 0.7357 val_loss= 1.3765 val_acc= 0.6367 time= 0.0164\n",
      "Epoch: 0069 train_loss= 1.1536 train_acc= 0.7571 val_loss= 1.3693 val_acc= 0.6367 time= 0.0170\n",
      "Epoch: 0070 train_loss= 1.1437 train_acc= 0.7643 val_loss= 1.3622 val_acc= 0.6467 time= 0.0212\n",
      "Epoch: 0071 train_loss= 1.1338 train_acc= 0.7857 val_loss= 1.3552 val_acc= 0.6500 time= 0.0211\n",
      "Epoch: 0072 train_loss= 1.1239 train_acc= 0.7857 val_loss= 1.3484 val_acc= 0.6533 time= 0.0237\n",
      "Epoch: 0073 train_loss= 1.1147 train_acc= 0.7857 val_loss= 1.3418 val_acc= 0.6533 time= 0.0237\n",
      "Epoch: 0074 train_loss= 1.1057 train_acc= 0.8000 val_loss= 1.3347 val_acc= 0.6567 time= 0.0212\n",
      "Epoch: 0075 train_loss= 1.0966 train_acc= 0.8000 val_loss= 1.3275 val_acc= 0.6567 time= 0.0179\n",
      "Epoch: 0076 train_loss= 1.0877 train_acc= 0.8000 val_loss= 1.3203 val_acc= 0.6567 time= 0.0360\n",
      "Epoch: 0077 train_loss= 1.0790 train_acc= 0.8000 val_loss= 1.3134 val_acc= 0.6567 time= 0.0185\n",
      "Epoch: 0078 train_loss= 1.0704 train_acc= 0.7929 val_loss= 1.3071 val_acc= 0.6567 time= 0.0190\n",
      "Epoch: 0079 train_loss= 1.0619 train_acc= 0.7929 val_loss= 1.3011 val_acc= 0.6633 time= 0.0198\n",
      "Epoch: 0080 train_loss= 1.0535 train_acc= 0.7857 val_loss= 1.2954 val_acc= 0.6733 time= 0.0169\n",
      "Epoch: 0081 train_loss= 1.0452 train_acc= 0.7857 val_loss= 1.2891 val_acc= 0.6733 time= 0.0171\n",
      "Epoch: 0082 train_loss= 1.0370 train_acc= 0.7857 val_loss= 1.2827 val_acc= 0.6767 time= 0.0173\n",
      "Epoch: 0083 train_loss= 1.0287 train_acc= 0.7857 val_loss= 1.2762 val_acc= 0.6800 time= 0.0156\n",
      "Epoch: 0084 train_loss= 1.0204 train_acc= 0.8143 val_loss= 1.2691 val_acc= 0.6767 time= 0.0157\n",
      "Epoch: 0085 train_loss= 1.0124 train_acc= 0.8143 val_loss= 1.2626 val_acc= 0.6800 time= 0.0208\n",
      "Epoch: 0086 train_loss= 1.0046 train_acc= 0.8143 val_loss= 1.2561 val_acc= 0.6800 time= 0.0160\n",
      "Epoch: 0087 train_loss= 0.9968 train_acc= 0.8143 val_loss= 1.2500 val_acc= 0.6800 time= 0.0166\n",
      "Epoch: 0088 train_loss= 0.9891 train_acc= 0.8143 val_loss= 1.2443 val_acc= 0.6800 time= 0.0161\n",
      "Epoch: 0089 train_loss= 0.9815 train_acc= 0.8143 val_loss= 1.2384 val_acc= 0.6800 time= 0.0161\n",
      "Epoch: 0090 train_loss= 0.9737 train_acc= 0.8143 val_loss= 1.2321 val_acc= 0.6833 time= 0.0158\n",
      "Epoch: 0091 train_loss= 0.9658 train_acc= 0.8214 val_loss= 1.2259 val_acc= 0.6867 time= 0.0165\n",
      "Epoch: 0092 train_loss= 0.9582 train_acc= 0.8214 val_loss= 1.2202 val_acc= 0.6933 time= 0.0173\n",
      "Epoch: 0093 train_loss= 0.9510 train_acc= 0.8214 val_loss= 1.2146 val_acc= 0.6933 time= 0.0163\n",
      "Epoch: 0094 train_loss= 0.9440 train_acc= 0.8214 val_loss= 1.2096 val_acc= 0.6867 time= 0.0159\n",
      "Epoch: 0095 train_loss= 0.9371 train_acc= 0.8143 val_loss= 1.2049 val_acc= 0.6867 time= 0.0159\n",
      "Epoch: 0096 train_loss= 0.9296 train_acc= 0.8286 val_loss= 1.1997 val_acc= 0.7000 time= 0.0158\n",
      "Epoch: 0097 train_loss= 0.9222 train_acc= 0.8357 val_loss= 1.1943 val_acc= 0.7033 time= 0.0180\n",
      "Epoch: 0098 train_loss= 0.9151 train_acc= 0.8500 val_loss= 1.1891 val_acc= 0.7067 time= 0.0180\n",
      "Epoch: 0099 train_loss= 0.9081 train_acc= 0.8500 val_loss= 1.1836 val_acc= 0.7033 time= 0.0180\n",
      "Epoch: 0100 train_loss= 0.9014 train_acc= 0.8571 val_loss= 1.1777 val_acc= 0.7067 time= 0.0165\n",
      "Epoch: 0101 train_loss= 0.8948 train_acc= 0.8571 val_loss= 1.1721 val_acc= 0.7100 time= 0.0159\n",
      "Epoch: 0102 train_loss= 0.8884 train_acc= 0.8571 val_loss= 1.1670 val_acc= 0.7133 time= 0.0166\n",
      "Epoch: 0103 train_loss= 0.8820 train_acc= 0.8571 val_loss= 1.1616 val_acc= 0.7133 time= 0.0163\n",
      "Epoch: 0104 train_loss= 0.8752 train_acc= 0.8571 val_loss= 1.1556 val_acc= 0.7167 time= 0.0162\n",
      "Epoch: 0105 train_loss= 0.8684 train_acc= 0.8571 val_loss= 1.1495 val_acc= 0.7233 time= 0.0167\n",
      "Epoch: 0106 train_loss= 0.8620 train_acc= 0.8571 val_loss= 1.1437 val_acc= 0.7200 time= 0.0180\n",
      "Epoch: 0107 train_loss= 0.8562 train_acc= 0.8571 val_loss= 1.1388 val_acc= 0.7267 time= 0.0171\n",
      "Epoch: 0108 train_loss= 0.8509 train_acc= 0.8571 val_loss= 1.1351 val_acc= 0.7233 time= 0.0224\n",
      "Epoch: 0109 train_loss= 0.8453 train_acc= 0.8643 val_loss= 1.1315 val_acc= 0.7200 time= 0.0189\n",
      "Epoch: 0110 train_loss= 0.8392 train_acc= 0.8643 val_loss= 1.1281 val_acc= 0.7233 time= 0.0204\n",
      "Epoch: 0111 train_loss= 0.8329 train_acc= 0.8643 val_loss= 1.1232 val_acc= 0.7333 time= 0.0221\n",
      "Epoch: 0112 train_loss= 0.8270 train_acc= 0.8643 val_loss= 1.1187 val_acc= 0.7367 time= 0.0173\n",
      "Epoch: 0113 train_loss= 0.8212 train_acc= 0.8643 val_loss= 1.1137 val_acc= 0.7367 time= 0.0210\n",
      "Epoch: 0114 train_loss= 0.8153 train_acc= 0.8714 val_loss= 1.1078 val_acc= 0.7333 time= 0.0183\n",
      "Epoch: 0115 train_loss= 0.8095 train_acc= 0.8714 val_loss= 1.1018 val_acc= 0.7300 time= 0.0195\n",
      "Epoch: 0116 train_loss= 0.8039 train_acc= 0.8714 val_loss= 1.0958 val_acc= 0.7333 time= 0.0219\n",
      "Epoch: 0117 train_loss= 0.7985 train_acc= 0.8714 val_loss= 1.0901 val_acc= 0.7500 time= 0.0206\n",
      "Epoch: 0118 train_loss= 0.7928 train_acc= 0.8786 val_loss= 1.0850 val_acc= 0.7500 time= 0.0217\n",
      "Epoch: 0119 train_loss= 0.7869 train_acc= 0.8786 val_loss= 1.0798 val_acc= 0.7533 time= 0.0193\n",
      "Epoch: 0120 train_loss= 0.7810 train_acc= 0.8786 val_loss= 1.0750 val_acc= 0.7533 time= 0.0188\n",
      "Epoch: 0121 train_loss= 0.7753 train_acc= 0.8786 val_loss= 1.0708 val_acc= 0.7500 time= 0.0206\n",
      "Epoch: 0122 train_loss= 0.7699 train_acc= 0.8786 val_loss= 1.0670 val_acc= 0.7467 time= 0.0170\n",
      "Epoch: 0123 train_loss= 0.7648 train_acc= 0.8786 val_loss= 1.0632 val_acc= 0.7500 time= 0.0190\n",
      "Epoch: 0124 train_loss= 0.7598 train_acc= 0.8786 val_loss= 1.0602 val_acc= 0.7500 time= 0.0176\n",
      "Epoch: 0125 train_loss= 0.7547 train_acc= 0.8786 val_loss= 1.0568 val_acc= 0.7500 time= 0.0177\n",
      "Epoch: 0126 train_loss= 0.7495 train_acc= 0.8786 val_loss= 1.0533 val_acc= 0.7533 time= 0.0173\n",
      "Epoch: 0127 train_loss= 0.7443 train_acc= 0.8857 val_loss= 1.0502 val_acc= 0.7567 time= 0.0163\n",
      "Epoch: 0128 train_loss= 0.7395 train_acc= 0.8929 val_loss= 1.0469 val_acc= 0.7567 time= 0.0164\n",
      "Epoch: 0129 train_loss= 0.7348 train_acc= 0.8929 val_loss= 1.0447 val_acc= 0.7567 time= 0.0176\n",
      "Epoch: 0130 train_loss= 0.7300 train_acc= 0.9000 val_loss= 1.0415 val_acc= 0.7600 time= 0.0164\n",
      "Epoch: 0131 train_loss= 0.7254 train_acc= 0.9000 val_loss= 1.0372 val_acc= 0.7633 time= 0.0165\n",
      "Epoch: 0132 train_loss= 0.7211 train_acc= 0.9000 val_loss= 1.0327 val_acc= 0.7633 time= 0.0165\n",
      "Epoch: 0133 train_loss= 0.7171 train_acc= 0.9000 val_loss= 1.0281 val_acc= 0.7700 time= 0.0214\n",
      "Epoch: 0134 train_loss= 0.7135 train_acc= 0.9000 val_loss= 1.0239 val_acc= 0.7667 time= 0.0228\n",
      "Epoch: 0135 train_loss= 0.7098 train_acc= 0.9071 val_loss= 1.0209 val_acc= 0.7733 time= 0.0194\n",
      "Epoch: 0136 train_loss= 0.7060 train_acc= 0.9071 val_loss= 1.0186 val_acc= 0.7767 time= 0.0206\n",
      "Epoch: 0137 train_loss= 0.7023 train_acc= 0.9143 val_loss= 1.0167 val_acc= 0.7800 time= 0.0196\n",
      "Epoch: 0138 train_loss= 0.6987 train_acc= 0.9143 val_loss= 1.0143 val_acc= 0.7767 time= 0.0196\n",
      "Epoch: 0139 train_loss= 0.6949 train_acc= 0.9143 val_loss= 1.0122 val_acc= 0.7767 time= 0.0202\n",
      "Epoch: 0140 train_loss= 0.6910 train_acc= 0.9143 val_loss= 1.0093 val_acc= 0.7800 time= 0.0186\n",
      "Epoch: 0141 train_loss= 0.6866 train_acc= 0.9143 val_loss= 1.0056 val_acc= 0.7800 time= 0.0214\n",
      "Epoch: 0142 train_loss= 0.6827 train_acc= 0.9071 val_loss= 1.0021 val_acc= 0.7767 time= 0.0166\n",
      "Epoch: 0143 train_loss= 0.6795 train_acc= 0.9071 val_loss= 0.9988 val_acc= 0.7733 time= 0.0204\n",
      "Epoch: 0144 train_loss= 0.6763 train_acc= 0.9143 val_loss= 0.9950 val_acc= 0.7633 time= 0.0189\n",
      "Epoch: 0145 train_loss= 0.6728 train_acc= 0.9071 val_loss= 0.9916 val_acc= 0.7667 time= 0.0158\n",
      "Epoch: 0146 train_loss= 0.6693 train_acc= 0.9071 val_loss= 0.9886 val_acc= 0.7700 time= 0.0164\n",
      "Epoch: 0147 train_loss= 0.6652 train_acc= 0.9071 val_loss= 0.9848 val_acc= 0.7767 time= 0.0163\n",
      "Epoch: 0148 train_loss= 0.6609 train_acc= 0.9000 val_loss= 0.9818 val_acc= 0.7767 time= 0.0164\n",
      "Epoch: 0149 train_loss= 0.6567 train_acc= 0.9071 val_loss= 0.9797 val_acc= 0.7800 time= 0.0168\n",
      "Epoch: 0150 train_loss= 0.6528 train_acc= 0.9143 val_loss= 0.9773 val_acc= 0.7800 time= 0.0160\n",
      "Epoch: 0151 train_loss= 0.6491 train_acc= 0.9214 val_loss= 0.9749 val_acc= 0.7767 time= 0.0159\n",
      "Epoch: 0152 train_loss= 0.6453 train_acc= 0.9214 val_loss= 0.9728 val_acc= 0.7833 time= 0.0174\n",
      "Epoch: 0153 train_loss= 0.6413 train_acc= 0.9214 val_loss= 0.9707 val_acc= 0.7867 time= 0.0157\n",
      "Epoch: 0154 train_loss= 0.6377 train_acc= 0.9214 val_loss= 0.9685 val_acc= 0.7900 time= 0.0163\n",
      "Epoch: 0155 train_loss= 0.6342 train_acc= 0.9214 val_loss= 0.9662 val_acc= 0.7900 time= 0.0206\n",
      "Epoch: 0156 train_loss= 0.6307 train_acc= 0.9214 val_loss= 0.9635 val_acc= 0.7900 time= 0.0170\n",
      "Epoch: 0157 train_loss= 0.6272 train_acc= 0.9214 val_loss= 0.9606 val_acc= 0.7900 time= 0.0163\n",
      "Epoch: 0158 train_loss= 0.6238 train_acc= 0.9214 val_loss= 0.9566 val_acc= 0.7900 time= 0.0168\n",
      "Epoch: 0159 train_loss= 0.6205 train_acc= 0.9214 val_loss= 0.9526 val_acc= 0.7900 time= 0.0159\n",
      "Epoch: 0160 train_loss= 0.6177 train_acc= 0.9214 val_loss= 0.9486 val_acc= 0.7900 time= 0.0164\n",
      "Epoch: 0161 train_loss= 0.6146 train_acc= 0.9214 val_loss= 0.9446 val_acc= 0.7867 time= 0.0157\n",
      "Epoch: 0162 train_loss= 0.6114 train_acc= 0.9214 val_loss= 0.9414 val_acc= 0.7867 time= 0.0172\n",
      "Epoch: 0163 train_loss= 0.6079 train_acc= 0.9214 val_loss= 0.9380 val_acc= 0.7900 time= 0.0176\n",
      "Epoch: 0164 train_loss= 0.6052 train_acc= 0.9286 val_loss= 0.9346 val_acc= 0.7933 time= 0.0167\n",
      "Epoch: 0165 train_loss= 0.6017 train_acc= 0.9286 val_loss= 0.9321 val_acc= 0.7933 time= 0.0162\n",
      "Epoch: 0166 train_loss= 0.5988 train_acc= 0.9286 val_loss= 0.9302 val_acc= 0.7933 time= 0.0172\n",
      "Epoch: 0167 train_loss= 0.5960 train_acc= 0.9286 val_loss= 0.9283 val_acc= 0.7933 time= 0.0210\n",
      "Epoch: 0168 train_loss= 0.5926 train_acc= 0.9286 val_loss= 0.9275 val_acc= 0.7933 time= 0.0167\n",
      "Epoch: 0169 train_loss= 0.5891 train_acc= 0.9286 val_loss= 0.9266 val_acc= 0.7900 time= 0.0176\n",
      "Epoch: 0170 train_loss= 0.5851 train_acc= 0.9286 val_loss= 0.9246 val_acc= 0.7933 time= 0.0179\n",
      "Epoch: 0171 train_loss= 0.5809 train_acc= 0.9214 val_loss= 0.9210 val_acc= 0.8000 time= 0.0166\n",
      "Epoch: 0172 train_loss= 0.5767 train_acc= 0.9214 val_loss= 0.9170 val_acc= 0.8000 time= 0.0169\n",
      "Epoch: 0173 train_loss= 0.5732 train_acc= 0.9357 val_loss= 0.9134 val_acc= 0.7967 time= 0.0175\n",
      "Epoch: 0174 train_loss= 0.5703 train_acc= 0.9357 val_loss= 0.9108 val_acc= 0.7933 time= 0.0163\n",
      "Epoch: 0175 train_loss= 0.5678 train_acc= 0.9357 val_loss= 0.9077 val_acc= 0.7933 time= 0.0209\n",
      "Epoch: 0176 train_loss= 0.5655 train_acc= 0.9357 val_loss= 0.9051 val_acc= 0.7933 time= 0.0210\n",
      "Epoch: 0177 train_loss= 0.5629 train_acc= 0.9357 val_loss= 0.9029 val_acc= 0.7900 time= 0.0214\n",
      "Epoch: 0178 train_loss= 0.5601 train_acc= 0.9357 val_loss= 0.9011 val_acc= 0.7900 time= 0.0181\n",
      "Epoch: 0179 train_loss= 0.5572 train_acc= 0.9357 val_loss= 0.8987 val_acc= 0.7967 time= 0.0186\n",
      "Epoch: 0180 train_loss= 0.5541 train_acc= 0.9357 val_loss= 0.8967 val_acc= 0.8000 time= 0.0190\n",
      "Epoch: 0181 train_loss= 0.5512 train_acc= 0.9429 val_loss= 0.8945 val_acc= 0.8000 time= 0.0181\n",
      "Epoch: 0182 train_loss= 0.5485 train_acc= 0.9429 val_loss= 0.8935 val_acc= 0.8000 time= 0.0204\n",
      "Epoch: 0183 train_loss= 0.5466 train_acc= 0.9429 val_loss= 0.8920 val_acc= 0.7967 time= 0.0182\n",
      "Epoch: 0184 train_loss= 0.5451 train_acc= 0.9429 val_loss= 0.8909 val_acc= 0.7900 time= 0.0201\n",
      "Epoch: 0185 train_loss= 0.5443 train_acc= 0.9500 val_loss= 0.8908 val_acc= 0.7900 time= 0.0180\n",
      "Epoch: 0186 train_loss= 0.5423 train_acc= 0.9500 val_loss= 0.8889 val_acc= 0.7900 time= 0.0190\n",
      "Epoch: 0187 train_loss= 0.5398 train_acc= 0.9500 val_loss= 0.8864 val_acc= 0.7900 time= 0.0179\n",
      "Epoch: 0188 train_loss= 0.5369 train_acc= 0.9429 val_loss= 0.8833 val_acc= 0.7867 time= 0.0164\n",
      "Epoch: 0189 train_loss= 0.5345 train_acc= 0.9429 val_loss= 0.8807 val_acc= 0.7900 time= 0.0187\n",
      "Epoch: 0190 train_loss= 0.5323 train_acc= 0.9429 val_loss= 0.8789 val_acc= 0.7900 time= 0.0182\n",
      "Epoch: 0191 train_loss= 0.5307 train_acc= 0.9429 val_loss= 0.8773 val_acc= 0.7933 time= 0.0165\n",
      "Epoch: 0192 train_loss= 0.5293 train_acc= 0.9429 val_loss= 0.8755 val_acc= 0.7967 time= 0.0166\n",
      "Epoch: 0193 train_loss= 0.5285 train_acc= 0.9429 val_loss= 0.8740 val_acc= 0.7967 time= 0.0160\n",
      "Epoch: 0194 train_loss= 0.5270 train_acc= 0.9429 val_loss= 0.8731 val_acc= 0.8033 time= 0.0168\n",
      "Epoch: 0195 train_loss= 0.5240 train_acc= 0.9429 val_loss= 0.8728 val_acc= 0.8067 time= 0.0182\n",
      "Epoch: 0196 train_loss= 0.5209 train_acc= 0.9429 val_loss= 0.8735 val_acc= 0.7967 time= 0.0226\n",
      "Epoch: 0197 train_loss= 0.5184 train_acc= 0.9500 val_loss= 0.8744 val_acc= 0.7867 time= 0.0230\n",
      "Epoch: 0198 train_loss= 0.5159 train_acc= 0.9500 val_loss= 0.8732 val_acc= 0.7833 time= 0.0205\n",
      "Epoch: 0199 train_loss= 0.5143 train_acc= 0.9500 val_loss= 0.8739 val_acc= 0.7867 time= 0.0240\n",
      "Epoch: 0200 train_loss= 0.5119 train_acc= 0.9500 val_loss= 0.8722 val_acc= 0.7900 time= 0.0196\n"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.9387 accuracy= 0.7690\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(test_loss[0]),\n",
    "      \"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFでXからyを予測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier()\n",
    "RF.fit(X[idx_train], y[idx_train])\n",
    "pred = RF.predict(X[idx_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 7)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ..., \n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ..., False,  True,  True],\n",
       "       [ True, False,  True, ...,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
